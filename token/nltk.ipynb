{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07d6ce7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, PunktTokenizer\n",
    "from nltk.tokenize import sent_tokenize, PunktSentenceTokenizer\n",
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer\n",
    "from nltk.tokenize import word_tokenize, WordPunctTokenizer\n",
    "from nltk.tokenize import regexp_tokenize, WhitespaceTokenizer, WordPunctTokenizer\n",
    "# pip install nltk\n",
    "# nltk.download()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca5a7f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\hi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('gutenberg')\n",
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76de1bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Hello world.', 'welcome AI.'], ['Hello world.', 'welcome AI.'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = 'Hello world. welcome AI.'\n",
    "sent_tokenize(p), PunktSentenceTokenizer().tokenize(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99de7cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Let', \"'s\", 'see', 'how', 'it', \"'s\", 'working'],\n",
       " ['Let', \"'s\", 'see', 'how', 'it', \"'s\", 'working'],\n",
       " ['Let', \"'\", 's', 'see', 'how', 'it', \"'\", 's', 'working'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = \"Let's see how it's working\"\n",
    "word_tokenize(p), TreebankWordTokenizer().tokenize(p), WordPunctTokenizer().tokenize(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99e70a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([\"'AI-이노베이션\", '센터', \"'\", '에', '오신', '것을', '환영합니다', '.'],\n",
       " [\"'AI-이노베이션\", \"센터'에\", '오신', '것을', '환영합니다', '.'],\n",
       " [\"'\", 'AI', '-', '이노베이션', '센터', \"'\", '에', '오신', '것을', '환영합니다', '.'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = \"'AI-이노베이션 센터'에 오신 것을 환영합니다.\"\n",
    "word_tokenize(p), TreebankWordTokenizer().tokenize(p), WordPunctTokenizer().tokenize(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0884b89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "C:\\Users\\hi\\AppData\\Local\\Temp\\ipykernel_18956\\2417425462.py:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  regexp_tokenize(p, \"[\\w']+\"), WordPunctTokenizer().tokenize(p)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Let's\", 'see', 'how', \"it's\", 'working.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = \"Let's see how it's working.\"\n",
    "regexp_tokenize(p, \"[\\w']+\"), WordPunctTokenizer().tokenize(p)\n",
    "WhitespaceTokenizer().tokenize(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff48b1e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158167, 17409, 7493, 7290, 191855, 8376, 161983, 7723)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "\n",
    "corpus = gutenberg.open(gutenberg.fileids()[0]).read()\n",
    "tokens1 = corpus.split()\n",
    "tokens2 = sent_tokenize(corpus)\n",
    "tokens3 = word_tokenize(corpus)\n",
    "tokens4 = regexp_tokenize(corpus, r'[\\w]+')\n",
    "len(tokens1), len(set(tokens1)), \\\n",
    "len(tokens2), len(set(tokens2)), \\\n",
    "len(tokens3), len(set(tokens3)), \\\n",
    "len(tokens4), len(set(tokens4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20e01f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((',', 12016), ((',', ','), 12016)),\n",
       " (('.', 6355), (('.', '.'), 6355)),\n",
       " (('to', 5125), (('to', 'TO'), 5125)),\n",
       " (('the', 4844), (('the', 'DT'), 4844)),\n",
       " (('and', 4653), (('and', 'CC'), 4653)),\n",
       " (('of', 4272), (('of', 'IN'), 4272)),\n",
       " (('I', 3177), (('I', 'PRP'), 3177)),\n",
       " (('--', 3100), (('--', ':'), 3100)),\n",
       " (('a', 3001), (('a', 'DT'), 3001)),\n",
       " ((\"''\", 2452), ((\"''\", \"''\"), 2452))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "corpus = gutenberg.open(gutenberg.fileids()[0]).read()\n",
    "tokens = word_tokenize(corpus)\n",
    "list(zip(FreqDist(tokens).most_common(10),\n",
    "         FreqDist(pos_tag(tokens)).most_common(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f384f658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c0a0d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4178, 2029, 4640, 2023, 10053, 1294)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.corpus import kolaw\n",
    "from konlpy.tag import Kkma\n",
    "\n",
    "corpus = kolaw.open(kolaw.fileids()[0]).read()\n",
    "tokens = word_tokenize(corpus)\n",
    "len(corpus.split()), len(set(corpus.split())), \\\n",
    "len(tokens), len(set(tokens)), \\\n",
    "len(Kkma().pos(corpus)), len(set(Kkma().pos(corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3898b8c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('.', 357), (('의', 'JKG'), 532)),\n",
       " ((',', 101), (('.', 'SF'), 359)),\n",
       " (('수', 87), (('하', 'XSV'), 350)),\n",
       " (('①', 75), (('에', 'JKM'), 328)),\n",
       " (('또는', 70), (('ㄴ다', 'EFN'), 243)),\n",
       " (('의하여', 66), (('ㄴ', 'ETD'), 234)),\n",
       " (('법률이', 57), (('을', 'JKO'), 211)),\n",
       " (('있다', 57), (('은', 'JX'), 182)),\n",
       " (('한다', 56), (('는', 'JX'), 180)),\n",
       " (('정하는', 50), (('저', 'NP'), 155))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = Kkma().pos\n",
    "\n",
    "corpus = kolaw.open(kolaw.fileids()[0]).read()\n",
    "tokens = word_tokenize(corpus)\n",
    "list(zip(FreqDist(tokens).most_common(10),\n",
    "         FreqDist(pos(corpus)).most_common(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71de5922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('나', 'NP'), ('는', 'JX'), ('밥', 'NNG'), ('을', 'JKO'), ('먹', 'VV'), ('는', 'EPT'), ('다', 'EFN'), ('.', 'SF'), ('하늘', 'NNG'), ('을', 'JKO'), ('날', 'VV'), ('는', 'ETD'), ('자동차', 'NNG')]\n",
      "[('나', 'NP'), ('는', 'JX'), ('밥', 'NNG'), ('을', 'JKO'), ('먹', 'VV'), ('는다', 'EF'), ('.', 'SF'), ('하늘', 'NNG'), ('을', 'JKO'), ('나', 'NP'), ('는', 'JX'), ('자동차', 'NNG')]\n",
      "[('나', 'N'), ('는', 'J'), ('밥', 'N'), ('을', 'J'), ('먹', 'P'), ('는다', 'E'), ('.', 'S'), ('하늘', 'N'), ('을', 'J'), ('나', 'N'), ('는', 'J'), ('자동차', 'N')]\n",
      "[('나', 'Noun'), ('는', 'Josa'), ('밥', 'Noun'), ('을', 'Josa'), ('먹는다', 'Verb'), ('.', 'Punctuation'), ('하늘', 'Noun'), ('을', 'Josa'), ('나', 'Noun'), ('는', 'Josa'), ('자동차', 'Noun')]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Komoran, Hannanum, Okt\n",
    "\n",
    "sentence = '나는 밥을 먹는다. 하늘을 나는 자동차'\n",
    "# kkma, komoran = 형태소에 대한 예약어\n",
    "print(Kkma().pos(sentence))\n",
    "print(Komoran().pos(sentence))\n",
    "# hannanum = 축약어\n",
    "print(Hannanum().pos(sentence))\n",
    "# okt = 한글 발음\n",
    "print(Okt().pos(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de1337f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('아버지', 'NNG'), ('가방', 'NNG'), ('에', 'JKM'), ('들어가', 'VV'), ('시', 'EPH'), ('ㄴ다', 'EFN')]\n",
      "[('아버지', 'NNG'), ('가방', 'NNP'), ('에', 'JKB'), ('들어가', 'VV'), ('시', 'EP'), ('ㄴ다', 'EC')]\n",
      "[('아버지가방에들어가', 'N'), ('이', 'J'), ('시ㄴ다', 'E')]\n",
      "[('아버지', 'Noun'), ('가방', 'Noun'), ('에', 'Josa'), ('들어가신다', 'Verb')]\n"
     ]
    }
   ],
   "source": [
    "sentence = '아버지가방에들어가신다'\n",
    "\n",
    "# 각 분석기에 따른 형태소 나누는 것의 차이\n",
    "print(Kkma().pos(sentence))\n",
    "print(Komoran().pos(sentence))\n",
    "print(Hannanum().pos(sentence))\n",
    "print(Okt().pos(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49d2af26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('아이', 'NNG'), ('폰', 'NNG'), ('애플', 'NNP'), ('공', 'NNG'), ('홈', 'NNG'), ('에서', 'JKM'), ('언락', 'NNG'), ('폰', 'NNG'), ('질르', 'VV'), ('어', 'ECS'), ('버리', 'VXV'), ('었', 'EPT'), ('다', 'ECS'), ('6', 'NR'), ('+', 'SW'), ('128', 'NR'), ('기가', 'NNG'), ('실버', 'NNG'), ('ㅋㅋㅋ', 'EMO')]\n",
      "[('아이폰', 'NNP'), ('애플', 'NNP'), ('공', 'NNP'), ('홈', 'NNG'), ('에서', 'JKB'), ('언', 'NNG'), ('락', 'NNG'), ('폰', 'NNG'), ('지르', 'VV'), ('어', 'EC'), ('버리', 'VX'), ('었', 'EP'), ('다', 'EC'), ('6', 'SN'), ('+', 'SW'), ('128기가실버ㅋㅋㅋ', 'NA')]\n",
      "[('아이폰', 'N'), ('애플공홈', 'N'), ('에서', 'J'), ('언락폰질러버렸다', 'N'), ('6+', 'N'), ('128기가실벜ㅋㅋ', 'N')]\n",
      "[('아이폰', 'Noun'), ('애플', 'Noun'), ('공홈', 'Noun'), ('에서', 'Josa'), ('언', 'Modifier'), ('락폰', 'Noun'), ('질러', 'Verb'), ('버렸다', 'Verb'), ('6', 'Number'), ('+', 'Punctuation'), ('128', 'Number'), ('기', 'Noun'), ('가', 'Josa'), ('실버', 'Noun'), ('ㅋㅋㅋ', 'KoreanParticle')]\n"
     ]
    }
   ],
   "source": [
    "sentence = '아이폰 애플공홈에서 언락폰질러버렸다 6+ 128기가실버ㅋㅋㅋ'\n",
    "\n",
    "print(Kkma().pos(sentence))\n",
    "print(Komoran().pos(sentence))\n",
    "print(Hannanum().pos(sentence))\n",
    "print(Okt().pos(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "651a3ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['아이폰', '폰애플', '애플공', '공홈', '홈언락', '언락폰']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def eojeol(sentence, n=2):\n",
    "    ngram =list()\n",
    "    tokens = sentence.split()\n",
    "    for i in range(len(tokens)-(n-1)):\n",
    "        ngram.append(''.join(tokens[i:i+n]))\n",
    "    return ngram\n",
    "sentence = '아이폰 애플 공홈에서 언락폰 질러버렸다'\n",
    "eojeol(sentence)\n",
    "eojeol(' '.join(Kkma().morphs(sentence)))\n",
    "eojeol(' '.join([_[0] for _ in Kkma().pos(sentence) if _[1][0] == 'N']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e78a6ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['아이폰', '이폰_', '폰_애', '_애플', '애플_', '플_공', '_공홈', '공홈에', '홈에서', '에서_', '서_언', '_언락', '언락폰', '락폰_', '폰_질', '_질러', '질러버', '러버렸', '버렸다']\n",
      "['아이_', '이_폰', '_폰_', '폰_애', '_애플', '애플_', '플_공', '_공_', '공_홈', '_홈_', '홈_에', '_에서', '에서_', '서_언', '_언락', '언락_', '락_폰', '_폰_', '폰_지', '_지르', '지르_', '르_어', '_어_', '어_버', '_버리', '버리_', '리_었', '_었_', '었_다']\n",
      "['아이_', '이_폰', '_폰_', '폰_애', '_애플', '애플_', '플_공', '_공_', '공_홈', '_홈_', '홈_언', '_언락', '언락_', '락_폰']\n"
     ]
    }
   ],
   "source": [
    "def umjeol(sentence, n=2):\n",
    "    ngram = list()\n",
    "    tokens = list(sentence)\n",
    "    for i in range(len(tokens)-(n-1)):\n",
    "        ngram.append(''.join(tokens[i:i+n]))\n",
    "    return ngram\n",
    "\n",
    "print(umjeol(sentence.replace(' ','_'), 3))\n",
    "print(umjeol('_'.join(Kkma().morphs(sentence)), 3))\n",
    "print(umjeol('_'.join([_[0] for _ in Kkma().pos(sentence) if _[1][0] == 'N']), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47faf791",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
